"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.uploadPackfile = exports.uploadPackfiles = exports.uploadToGitDB = void 0;
const child_process_1 = __importDefault(require("child_process"));
const fs_1 = __importDefault(require("fs"));
const promises_1 = require("fs/promises");
const os_1 = __importDefault(require("os"));
const form_data_1 = __importDefault(require("form-data"));
const semver_1 = require("semver");
const upath_1 = __importDefault(require("upath"));
const get_git_data_1 = require("../../helpers/git/get-git-data");
const retry_1 = require("../../helpers/retry");
const API_TIMEOUT = 15000;
// we only consider recent commits to avoid uploading the whole repository
// at most 1000 commits or > 1 month of data is considered.
const MAX_HISTORY = {
    maxCommits: 1000,
    oldestCommits: '1 month ago',
};
const getCommitsToInclude = (log, request, git, repositoryURL) => __awaiter(void 0, void 0, void 0, function* () {
    let latestCommits;
    try {
        latestCommits = yield getLatestLocalCommits(git);
        if (latestCommits.length === 0) {
            log.debug('No local commits found.');
            return {
                commitsToInclude: [],
                commitsToExclude: [],
                headCommit: '',
            };
        }
        log.debug(`${latestCommits.length} commits found, asking GitDB which ones are missing.`);
    }
    catch (err) {
        log.warn(`Failed getting local commits: ${err}`);
        throw err;
    }
    let commitsToExclude;
    try {
        commitsToExclude = yield getKnownCommits(log, request, repositoryURL, latestCommits);
        log.debug(`${commitsToExclude.length} commits already in GitDB.`);
    }
    catch (err) {
        log.warn(`Failed getting commits to exclude: ${err}`);
        throw err;
    }
    return {
        commitsToInclude: latestCommits.filter((x) => !commitsToExclude.includes(x)),
        commitsToExclude,
        headCommit: latestCommits[0],
    };
});
const uploadToGitDB = (log, request, git, dryRun, repositoryURL) => __awaiter(void 0, void 0, void 0, function* () {
    let repoURL;
    if (repositoryURL) {
        repoURL = repositoryURL;
    }
    else {
        try {
            repoURL = yield (0, get_git_data_1.gitRemote)(git);
            log.debug(`Syncing repository ${repoURL}`);
        }
        catch (err) {
            log.warn(`Failed getting repository URL: ${err}`);
            throw err;
        }
    }
    let commitsToInclude;
    let commitsToExclude;
    let headCommit;
    const getCommitsBeforeUnshallowing = yield getCommitsToInclude(log, request, git, repoURL);
    commitsToInclude = getCommitsBeforeUnshallowing.commitsToInclude;
    commitsToExclude = getCommitsBeforeUnshallowing.commitsToExclude;
    headCommit = getCommitsBeforeUnshallowing.headCommit;
    // If there are no commits to include, it means the backend already has all the commits.
    if (commitsToInclude.length === 0) {
        return;
    }
    // If there are commits to include and the repository is shallow, we need to repeat the process after unshallowing
    const isShallow = yield isShallowRepository(git);
    if (isShallow) {
        yield unshallowRepository(log, git);
        const getCommitsAfterUnshallowing = yield getCommitsToInclude(log, request, git, repoURL);
        commitsToInclude = getCommitsAfterUnshallowing.commitsToInclude;
        commitsToExclude = getCommitsAfterUnshallowing.commitsToExclude;
        headCommit = getCommitsBeforeUnshallowing.headCommit;
    }
    // Get the list of all objects (commits, trees) to upload. This list can be quite long
    // so quite memory intensive (multiple MBs).
    let objectsToUpload;
    try {
        objectsToUpload = yield getObjectsToUpload(git, commitsToInclude, commitsToExclude);
        log.debug(`${objectsToUpload.length} objects to upload.`);
    }
    catch (err) {
        log.warn(`Failed getting objects to upload: ${err}`);
        throw err;
    }
    let packfiles;
    let tmpDir;
    try {
        ;
        [packfiles, tmpDir] = yield generatePackFilesForCommits(log, objectsToUpload);
        log.debug(`${packfiles.length} packfiles generated.`);
    }
    catch (err) {
        log.warn(`Failed generating packfiles: ${err}`);
        throw err;
    }
    try {
        if (dryRun) {
            log.debug(`Dry-run enabled, not uploading anything.`);
            return;
        }
        log.debug(`Uploading packfiles...`);
        yield (0, exports.uploadPackfiles)(log, request, repoURL, headCommit, packfiles);
        log.debug('Successfully uploaded packfiles.');
    }
    catch (err) {
        log.warn(`Failed to upload packfiles: ${err}`);
        throw err;
    }
    finally {
        if (tmpDir !== undefined) {
            fs_1.default.rmSync(tmpDir, { recursive: true });
        }
    }
});
exports.uploadToGitDB = uploadToGitDB;
const getLatestLocalCommits = (git) => __awaiter(void 0, void 0, void 0, function* () {
    // we add some boundaries to avoid retrieving ALL commits here.
    const logResult = yield git.log([`-n ${MAX_HISTORY.maxCommits}`, `--since="${MAX_HISTORY.oldestCommits}"`]);
    return logResult.all.map((c) => c.hash);
});
const isShallowRepository = (git) => __awaiter(void 0, void 0, void 0, function* () {
    const gitversion = String(yield git.version());
    if ((0, semver_1.lte)(gitversion, '2.27.0')) {
        return false;
    }
    return (yield git.revparse('--is-shallow-repository')) === 'true';
});
const unshallowRepository = (log, git) => __awaiter(void 0, void 0, void 0, function* () {
    log.info('[unshallow] Git repository is a shallow clone, unshallowing it...');
    const [headCommit, remoteName] = yield Promise.all([git.revparse('HEAD'), (0, get_git_data_1.getDefaultRemoteName)(git)]);
    const baseCommandLogLine = `[unshallow] Running git fetch --shallow-since="${MAX_HISTORY.oldestCommits}" --update-shallow --filter=blob:none --recurse-submodules=no`;
    log.info(`${baseCommandLogLine} $(git config --default origin --get clone.defaultRemoteName) $(git rev-parse HEAD)`);
    try {
        yield git.fetch([
            `--shallow-since="${MAX_HISTORY.oldestCommits}"`,
            '--update-shallow',
            '--filter=blob:none',
            '--recurse-submodules=no',
            remoteName,
            headCommit,
        ]);
    }
    catch (err) {
        // If the local HEAD is a commit that has not been pushed to the remote, the above command will fail.
        log.warn(`[unshallow] Failed to unshallow: ${err}`);
        try {
            log.info(`${baseCommandLogLine} $(git config --default origin --get clone.defaultRemoteName) $(git rev-parse --abbrev-ref --symbolic-full-name @{upstream})`);
            const upstreamRemote = yield git.revparse('--abbrev-ref --symbolic-full-name @{upstream}');
            yield git.fetch([
                `--shallow-since="${MAX_HISTORY.oldestCommits}"`,
                '--update-shallow',
                '--filter=blob:none',
                '--recurse-submodules=no',
                remoteName,
                upstreamRemote,
            ]);
        }
        catch (secondError) {
            // If the CI is working on a detached HEAD or branch tracking hasnâ€™t been set up, the above command will fail.
            log.warn(`[unshallow] Failed to unshallow again: ${secondError}`);
            log.info(`${baseCommandLogLine} $(git config --default origin --get clone.defaultRemoteName)`);
            yield git.fetch([
                `--shallow-since="${MAX_HISTORY.oldestCommits}"`,
                '--update-shallow',
                '--filter=blob:none',
                '--recurse-submodules=no',
                remoteName,
            ]);
        }
    }
    log.info('[unshallow] Fetch completed.');
});
// getKnownCommits asks the backend which of the given commits are already known
const getKnownCommits = (log, request, repoURL, latestCommits) => __awaiter(void 0, void 0, void 0, function* () {
    const localCommitData = JSON.stringify({
        meta: {
            repository_url: repoURL,
        },
        data: latestCommits.map((commit) => ({
            id: commit,
            type: 'commit',
        })),
    });
    const response = yield runRequest(log, 'search_commits', () => request({
        url: '/api/v2/git/repository/search_commits',
        headers: {
            'Content-Type': 'application/json',
        },
        data: localCommitData,
        method: 'POST',
        timeout: API_TIMEOUT,
    }));
    const commits = response.data;
    if (!commits || commits.data === undefined) {
        throw new Error(`Invalid API response: ${response}`);
    }
    return commits.data.map((c) => {
        if (c.type !== 'commit' || c.id === undefined) {
            throw new Error('Invalid commit type response');
        }
        return validateCommit(c.id);
    });
});
const validateCommit = (sha) => {
    const isValidSha1 = (s) => /^[0-9a-f]{40}$/.test(s);
    const isValidSha256 = (s) => /^[0-9a-f]{64}$/.test(s);
    if (!isValidSha1(sha) && !isValidSha256(sha)) {
        throw new Error(`Invalid commit format: ${sha}`);
    }
    return sha;
};
const getObjectsToUpload = (git, commitsToInclude, commitsToExclude) => __awaiter(void 0, void 0, void 0, function* () {
    const rawResponse = yield git.raw(['rev-list', '--objects', '--no-object-names', '--filter=blob:none', `--since="${MAX_HISTORY.oldestCommits}"`]
        .concat(commitsToExclude.map((sha) => '^' + sha))
        .concat(commitsToInclude));
    const objectsToInclude = rawResponse.split('\n').filter((c) => c !== '');
    return objectsToInclude;
});
const generatePackFilesForCommits = (log, commits) => __awaiter(void 0, void 0, void 0, function* () {
    if (commits.length === 0) {
        return [[], undefined];
    }
    const generatePackfiles = (baseTmpPath) => __awaiter(void 0, void 0, void 0, function* () {
        const randomPrefix = String(Math.floor(Math.random() * 10000));
        const tmpPath = yield (0, promises_1.mkdtemp)(upath_1.default.join(baseTmpPath, 'dd-packfiles-'));
        const packfilePath = upath_1.default.join(tmpPath, randomPrefix);
        const packObjectResults = child_process_1.default
            .execSync(`git pack-objects --compression=9 --max-pack-size=3m ${packfilePath}`, {
            input: commits.join('\n'),
        })
            .toString()
            .split('\n')
            .filter((sha) => sha.length > 0)
            .map((sha) => `${packfilePath}-${sha}.pack`);
        return [packObjectResults, tmpPath];
    });
    // Try using tmp folder first:
    try {
        return yield generatePackfiles(os_1.default.tmpdir());
    }
    catch (err) {
        /**
         * The generation of pack files in the temporary folder (from `os.tmpdir()`)
         * sometimes fails in certain CI setups with the error message
         * `unable to rename temporary pack file: Invalid cross-device link`.
         * The reason why is unclear.
         *
         * A workaround is to attempt to generate the pack files in `process.cwd()`.
         * While this works most of the times, it's not ideal since it affects the git status.
         * This workaround is intended to be temporary.
         *
         * TODO: fix issue and remove workaround.
         */
        log.warn(`Failed generation of packfiles in tmpdir: ${err}`);
        log.warn(`Generating them in ${process.cwd()} instead`);
        return generatePackfiles(process.cwd());
    }
});
const uploadPackfiles = (log, request, repoURL, headCommit, packfilePaths) => __awaiter(void 0, void 0, void 0, function* () {
    // this loop makes sure requests are performed sequentially
    for (const pack of packfilePaths) {
        yield (0, exports.uploadPackfile)(log, request, repoURL, headCommit, pack);
    }
});
exports.uploadPackfiles = uploadPackfiles;
const uploadPackfile = (log, request, repoURL, headCommit, packfilePath) => __awaiter(void 0, void 0, void 0, function* () {
    const pushedSha = JSON.stringify({
        data: {
            id: headCommit,
            type: 'commit',
        },
        meta: {
            repository_url: repoURL,
        },
    });
    const form = new form_data_1.default();
    form.append('pushedSha', pushedSha, { contentType: 'application/json' });
    const packFileContent = fs_1.default.readFileSync(packfilePath);
    // The original filename includes a random prefix, so we remove it here
    const [, filename] = upath_1.default.basename(packfilePath).split('-');
    form.append('packfile', packFileContent, {
        filename,
        contentType: 'application/octet-stream',
    });
    return runRequest(log, 'packfile', () => request({
        url: '/api/v2/git/repository/packfile',
        headers: Object.assign({}, form.getHeaders()),
        timeout: API_TIMEOUT,
        data: form,
        method: 'POST',
    }));
});
exports.uploadPackfile = uploadPackfile;
// runRequest will run the passed request, with retries of retriable errors + logging of any retry attempt.
const runRequest = (log, reqName, request) => __awaiter(void 0, void 0, void 0, function* () {
    return (0, retry_1.retryRequest)(request, {
        retries: 2,
        onRetry: (e, attempt) => {
            let errorMessage = `${e}`;
            const maybeHttpError = e;
            if (maybeHttpError.response && maybeHttpError.response.statusText) {
                errorMessage = `${maybeHttpError.message} (${maybeHttpError.response.statusText})`;
            }
            log.warn(`[attempt ${attempt}] Retrying ${reqName} request: ${errorMessage}`);
        },
    });
});
//# sourceMappingURL=gitdb.js.map